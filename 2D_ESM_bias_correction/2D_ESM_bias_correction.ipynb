{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036de454-25a9-4b99-8283-d202955f3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "from osgeo import gdal\n",
    "from math import floor, ceil\n",
    "from pyproj import Proj\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "import dask\n",
    "import glob\n",
    "import time\n",
    "import xesmf as xe\n",
    "# import quantile_mapping as qm\n",
    "\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/glade/u/home/bkruyt/libraries/storylines/storylines')\n",
    "sys.path.append('/glade/u/home/bkruyt/libraries')\n",
    "sys.path.append('/glade/u/home/bkruyt/libraries/st_lines')\n",
    "sys.path\n",
    "import icar2gmet as i2g\n",
    "from storylines.tools import quantile_mapping as qm\n",
    "### If storylines gives an error , use py3_yifan kernel /conda env\n",
    "# alternatively ../Example_scripts/2D_quantile_map/quantile_mapping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec2a49df-6497-49ae-bf2e-a0626b932f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_latlon_b(ds,lon_str,lat_str,lon_dim,lat_dim):\n",
    "    #### Longitude East-West Stagger\n",
    "    diffWestEast    = ds[lon_str].diff(lon_dim)                         # take difference in longitudes across west-east (columns) dimension\n",
    "    diffWestEast    = np.array(diffWestEast)                            # assign this to numpy array\n",
    "    padding         = diffWestEast[:,-1].reshape(len(diffWestEast[:,-1]),1)  # get the last column of the differences and reshape so it can be appended\n",
    "    diffWestEastAll = np.append(diffWestEast, padding, axis=1)/2               # append last value to all_data\n",
    "\n",
    "    # dimensions now the same as ds\n",
    "    lon_b_orig      = ds[lon_str]-diffWestEastAll\n",
    "\n",
    "    # lon_b needs to be staggered - add to the final row to bound\n",
    "    last_add     = ds[lon_str][:,-1]+diffWestEastAll[:,-1]\n",
    "    last_add     = np.array(last_add)\n",
    "    last_add     = last_add.reshape(len(last_add),1)\n",
    "    lon_b_append = np.append(np.array(lon_b_orig),last_add,1)\n",
    "    last_add     = lon_b_append[0,:].reshape(1,len(lon_b_append[0,:]))\n",
    "    lon_b_append = np.append(last_add,lon_b_append,axis=0)\n",
    "\n",
    "\n",
    "    #### Latitude Stagger\n",
    "    diffSouthNorth=ds[lat_str].diff(lat_dim)                         # take difference in longitudes across west-east (columns) dimension\n",
    "    diffSouthNorth=np.array(diffSouthNorth)                            # assign this to numpy array\n",
    "    padding=diffSouthNorth[0,:].reshape(1,len(diffSouthNorth[0,:]))  # get the last column of the differences and reshape so it can be appended\n",
    "    diffSouthNorthAll = np.append(padding,diffSouthNorth,axis=0)/2               # append last value to all_data\n",
    "\n",
    "    # dimensions now the same as ds\n",
    "    lat_b_orig      = ds[lat_str]-diffSouthNorthAll\n",
    "\n",
    "    # lat_b needs to be staggered - add to the first row to bound\n",
    "    last_add     = ds[lat_str][0,:]+diffWestEastAll[0,:]\n",
    "    last_add     = np.array(last_add)\n",
    "    last_add     = last_add.reshape(1,len(last_add))\n",
    "    lat_b_append = np.append(last_add,np.array(lat_b_orig),axis=0)\n",
    "    last_add     = lat_b_append[:,-1].reshape(len(lat_b_append[:,-1]),1)\n",
    "    lat_b_append = np.append(lat_b_append,last_add,axis=1)\n",
    "\n",
    "    \n",
    "    grid_with_bounds = {'lon': ds[lon_str].values,\n",
    "                               'lat': ds[lat_str].values,\n",
    "                               'lon_b': lon_b_append,\n",
    "                               'lat_b': lat_b_append,\n",
    "                              }\n",
    "\n",
    "    return grid_with_bounds\n",
    "\n",
    "def get_latlon_b_rect(ds,lon_str,lat_str,lon_dim,lat_dim):\n",
    "    #### Longitude Stagger\n",
    "    diffWestEast    = ds[lon_str].diff(lon_dim)                         # take difference in longitudes across west-east (columns) dimension\n",
    "    diffWestEastArr = np.array(diffWestEast).reshape(len(diffWestEast),1)                            # assign this to numpy array\n",
    "    padding         = diffWestEastArr[-1].reshape(1,1)  # get the last column of the differences and reshape so it can be appended\n",
    "    diffWestEastAll = np.append(diffWestEastArr, padding, axis=0)/2               # append last value to all_data\n",
    "    # # dimensions now the same as ds\n",
    "    lon_b_orig      = ds[lon_str].values.reshape(len(ds[lon_str]),1)-diffWestEastAll\n",
    "\n",
    "    # lon_b needs to be staggered - add to the final row to bound\n",
    "    last_add        = ds[lon_str][-1].values+diffWestEastAll[-1]\n",
    "    last_add        = np.array(last_add).reshape(len(last_add),1)\n",
    "    last_add        = last_add.reshape(1,1)\n",
    "    lon_b_append    = np.append(np.array(lon_b_orig),last_add,0)\n",
    "\n",
    "    #### Latitude Stagger\n",
    "    diffSouthNorth    = ds[lat_str].diff(lat_dim)                         # take difference in latitudes across west-east (columns) dimension\n",
    "    diffSouthNorthArr = np.array(diffSouthNorth).reshape(len(diffSouthNorth),1)                            # assign this to numpy array\n",
    "    padding         = diffSouthNorthArr[-1].reshape(1,1)  # get the last column of the differences and reshape so it can be appended\n",
    "    diffSouthNorthAll = np.append(diffSouthNorthArr, padding, axis=0)/2               # append last value to all_data\n",
    "    # # dimensions now the same as ds\n",
    "    lat_b_orig      = ds[lat_str].values.reshape(len(ds[lat_str]),1)-diffSouthNorthAll\n",
    "\n",
    "    # lat_b needs to be staggered - add to the final row to bound\n",
    "    last_add        = ds[lat_str][-1].values+diffSouthNorthAll[-1]\n",
    "    last_add        = np.array(last_add).reshape(len(last_add),1)\n",
    "    last_add        = last_add.reshape(1,1)\n",
    "    lat_b_append    = np.append(np.array(lat_b_orig),last_add,0)\n",
    "\n",
    "    grid_with_bounds = {'lon': ds[lon_str],\n",
    "                               'lat': ds[lat_str],\n",
    "                               'lon_b': lon_b_append.reshape(len(lon_b_append),),\n",
    "                               'lat_b': lat_b_append.reshape(len(lat_b_append),),\n",
    "                              }\n",
    "    return grid_with_bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65f1d364-2de1-4f24-a6cd-46d3a3c4a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET paths and parameters  ####\n",
    "\n",
    "# ERAi\n",
    "# era_path='/glade/u/home/currierw/scratch/erai/convection/erai/clipped_by_month_convert_SST/'\n",
    "# era_path='/glade/scratch/bkruyt/erai/erai_greatlakes_month/' # not chronological because monthly + buffer\n",
    "era_path='/glade/scratch/gutmann/icar/forcing/erai_conus/monthly/'  # path with erai files (chronological) cp not corrected!! And CONUS wide...\n",
    "\n",
    "modelLs       = ['MIROC-ES2L']; i=0\n",
    "scenarios     = ['ssp370'] #['ssp585'] #\n",
    "\n",
    "raw_dir = '/glade/scratch/bkruyt/CMIP6/raw/' #MIROC-ES2L/historical' \n",
    "in_dir = '/glade/scratch/bkruyt/CMIP6/BC_3D_merged/' #MIROC-ES2L/historical' \n",
    "out_dir='/glade/scratch/bkruyt/CMIP6/BC_2D_3D/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83f4c919-3429-4375-81f9-a9b2ddc00304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERAi cp data has not been converted to mm/sec - converting now ....\n"
     ]
    }
   ],
   "source": [
    "# Load the ERAi data, crop it and correct the cp \n",
    "\n",
    "dsObs=xr.open_mfdataset(era_path + 'erai*.nc',combine='by_coords') \n",
    "\n",
    "\n",
    "if ('Note' not in dsObs['cp'].attrs.keys() or \n",
    "   dsObs['cp'].attrs['Note'] != 'Converted from mm/time step (6 hours) to mm/s by dividing by 21600'):\n",
    "    print( 'ERAi cp data has not been converted to mm/sec - converting now ....' )\n",
    "    dsObs['cp']=dsObs['cp']/21600\n",
    "    dsObs['cp']=dsObs['cp'].assign_attrs(\n",
    "        {'long_name': 'Convective Precipitation', \n",
    "         'units': 'mm/s','Note':'Converted from mm/time step (6 hours) to mm/s by dividing by 21600'}\n",
    "    )\n",
    "else:\n",
    "    print( 'no correction to ERAi cp' )\n",
    "    print( dsObs['cp'].attrs )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea6aa4-cc38-4b04-ab3d-cb51ed1662f8",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30d68db1-2ebd-4b66-88af-636478f984d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 MIROC-ES2L ssp370\n",
      "   GCM data loaded\n",
      "   Regridded\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "   BC cp\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "detrending\n",
      "kwargs: {'alpha': 0.4, 'beta': 0.4, 'extrapolate': '1to1', 'n_endpoints': 50}\n",
      "putting the trend back\n",
      "   BC SST\n",
      "   total BC time:  740.4907281398773 \n",
      "\n",
      "    2095-01-01T00 2099-12-31T18\n",
      "    /glade/scratch/bkruyt/CMIP6/BC_2D_3D/MIROC-ES2L_bias_corr_final_2095-2099.nc\n",
      "   merging and writing took  180.79066228866577\n",
      " \n",
      "    - - - - 2d Bias Correction DONE!  - - - - \n",
      "   total time:  922.5429365634918 \n",
      "\n",
      "CPU times: user 7min 45s, sys: 3min 23s, total: 11min 9s\n",
      "Wall time: 15min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Generic BC (fut and past)\n",
    "\n",
    "for i in range(0,len(modelLs)):\n",
    "    for scen in scenarios:\n",
    "        t0 =time.time()\n",
    "        print(str(i+1)+' '+modelLs[i] + ' ' + scen)\n",
    "\n",
    "        # Load the 3D bc GCM data, onto which we will add the SST and cp \n",
    "        dsFull = xr.open_mfdataset(in_dir + modelLs[i] +'/'+ scen +'/'+modelLs[i] + '*.nc',combine='by_coords').load()\n",
    "\n",
    "        # Get SST and cp from the original (raw) GCM data:\n",
    "        dsRaw = xr.open_mfdataset(raw_dir + modelLs[i] +'/'+ scen +'/'+modelLs[i] + '*.nc',combine='by_coords')\n",
    "        dsCp_in = dsRaw['prec'] #.sel(time=slice(t1,t2)) ## WHY slice??? not generic. \n",
    "        dsSST_in = dsRaw['SST'] #.sel(time=slice(t1,t2))\n",
    "        # dsCp_in=dsCp_in.sel(time=~dsCp_in.get_index(\"time\").duplicated())\n",
    "        # dsSST_in=dsSST_in.sel(time=~dsSST_in.get_index(\"time\").duplicated())\n",
    "        print(\"   GCM data loaded\")\n",
    "\n",
    "\n",
    "\n",
    "        # Need to do a interpolation here with xesmf\n",
    "        cp_grid_with_bounds   = get_latlon_b_rect(dsCp_in,lon_str='lon',lat_str='lat',lon_dim='lon',lat_dim='lat')\n",
    "        # sst_grid_with_bounds  = get_latlon_b_rect(dsSST_in,lon_str='lon',lat_str='lat',lon_dim='lon',lat_dim='lat')         # redundant, should be same grid?\n",
    "        erai_grid_with_bounds = get_latlon_b_rect(dsObs['sst'],lon_str='lon',lat_str='lat',lon_dim='lon',lat_dim='lat')\n",
    "        regridder = xe.Regridder(cp_grid_with_bounds, erai_grid_with_bounds, 'bilinear') # input grid, gird you want to resample to, method\n",
    "\n",
    "        dsCp_out = regridder(dsCp_in).load()\n",
    "        dsSST_out = regridder(dsSST_in).load()\n",
    "\n",
    "        dsObs['cp']  = dsObs['cp'].load()\n",
    "        dsObs['sst'] = dsObs['sst'].load()\n",
    "\n",
    "        # the reference dataset should always be historical:\n",
    "        if scen=='historical':\n",
    "            dsCp_ref=dsCp_out.sel(time=slice('1979-09-01','2019-08-31'))\n",
    "            dsSST_ref=dsSST_out.sel(time=slice('1979-09-01','2019-08-31'))\n",
    "        else:  # load the historical GCM, regrid, ...\n",
    "            dsRaw_ref = xr.open_mfdataset(raw_dir + modelLs[i] +'/historical/'+modelLs[i] + '*.nc',combine='by_coords')\n",
    "            dsCp_ref =  regridder(dsRaw_ref['prec']).sel(time=slice('1979-09-01','2019-08-31')).load()\n",
    "            dsSST_ref =  regridder(dsRaw_ref['SST']).sel(time=slice('1979-09-01','2019-08-31')).load()\n",
    "            del dsRaw_ref\n",
    "          \n",
    "        dsSST_out=dsSST_out.interpolate_na(dim='time')\n",
    "        print(\"   Regridded\")\n",
    "\n",
    "        # Bias Correct Convective Precipitation\n",
    "        daPrecQm=qm.quantile_mapping_by_group(  dsCp_out,\n",
    "                                                dsCp_ref, # this should always be hist period?\n",
    "                                                dsObs['cp'].sel(time=slice('1979-09-01','2019-08-31')),\n",
    "                                                grouper='time.month',detrend=False,use_ref_data=True,extrapolate='1to1',n_endpoints=50)\n",
    "        daPrecQm = xr.where(dsCp_out <= 0, 0, daPrecQm)\n",
    "        daPrecQm.attrs = dsCp_in.attrs\n",
    "        daCpQm=daPrecQm.to_dataset(name='cp').load()\n",
    "        # # check/ add attrs?\n",
    "        # print(daPrecQm.attrs)\n",
    "        print(\"   BC cp\")\n",
    "\n",
    "\n",
    "        # Bias Correct Sea Surface Temperature\n",
    "        daSSTQm=qm.quantile_mapping_by_group(   dsSST_out,\n",
    "                                                dsSST_ref,\n",
    "                                                dsObs['sst'].sel(time=slice('1979-09-01','2019-08-31')),\n",
    "                                                grouper='time.month',detrend=True,use_ref_data=True,extrapolate='1to1',n_endpoints=50)\n",
    "        daSSTQm.attrs = dsSST_in.attrs\n",
    "        daSSTQm=daSSTQm.to_dataset(name='tskin').load()\n",
    "        print(\"   BC SST\")\n",
    "        print(\"   total BC time: \", time.time()-t0, \"\\n\") \n",
    "        \n",
    "        if scen == 'historical':\n",
    "            # time_s = np.arange(1950, 2015, 10)\n",
    "            # time_f = np.arange(1959, 2099, 10)\n",
    "            time_s = np.arange(1950, 2015, 10)\n",
    "            time_f = np.append(np.arange(1959, 2010, 10), 2014)\n",
    "        else:\n",
    "            # time_s = np.arange(2015, 2099, 10)\n",
    "            # time_f = np.arange(2024, 2100, 10)\n",
    "            time_s = np.append(2015, np.arange(2020, 2099, 10) )\n",
    "            time_f = np.append(2019, np.arange(2029, 2100, 10) )\n",
    "\n",
    "        if not os.path.exists(out_dir+'/'+modelLs[i]+'/'+scen):\n",
    "            os.makedirs(out_dir+'/'+modelLs[i]+'/'+scen)\n",
    "\n",
    "            \n",
    "        # TMP:\n",
    "        time_s=[2095]; time_f=[2099]; t=0\n",
    "        \n",
    "        # save the result in 10 year chunks:\n",
    "        for t in range(0,len(time_s)):\n",
    "        \n",
    "            \n",
    "            t1=time.time()\n",
    "\n",
    "            dsOut=xr.merge([#dsGCM_bc.sel(time=slice(time_s[t],time_f[t])),\n",
    "                            dsFull.sel(time=slice(str(time_s[t]),str(time_f[t]))),\n",
    "                            daCpQm.sel(time=slice(str(time_s[t]),str(time_f[t]))),\n",
    "                            daSSTQm.sel(time=slice(str(time_s[t]),str(time_f[t])))])\n",
    "\n",
    "            start_year = str(time_s[t])[0:4]\n",
    "            end_year   = str(time_f[t])[0:4]\n",
    "            print('   ', dsOut.time.values.min().astype('datetime64[h]'), dsOut.time.values.max().astype('datetime64[h]') )\n",
    "            dsOut.to_netcdf(out_dir+'/'+modelLs[i]+'/'+scen+'/' + modelLs[i] + '_bias_corr_final_'+start_year+'-'+end_year+'.nc')\n",
    "            print('   ', out_dir + modelLs[i]+'_bias_corr_final_'+start_year+'-'+end_year+'.nc')\n",
    "            print('   merging and writing took ',time.time()-t1 )\n",
    "               \n",
    "            del dsOut\n",
    " \n",
    "        print(\" \\n    - - - - 2d Bias Correction DONE!  - - - - \") \n",
    "        print(\"   total time: \", time.time()-t0, \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3852c-a6d5-491c-a197-3b19b42742ae",
   "metadata": {},
   "source": [
    "### Paralllel attempt... fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6a06f79-2848-494d-a6d5-298542403010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### PArallel version\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81202525-0e9c-4e6e-8796-e0a761cc0c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_to_file(dsFull, daCpQm, daSSTQm, time_s ):\n",
    "\n",
    "        t1=time.time()\n",
    "\n",
    "        time_f=time_s+9\n",
    "\n",
    "        dsOut=xr.merge([dsFull.sel(time=slice(str(time_s[t]),str(time_f[t]))),\n",
    "                        daCpQm.sel(time=slice(str(time_s[t]),str(time_f[t]))),\n",
    "                        daSSTQm.sel(time=slice(str(time_s[t]),str(time_f[t])))])\n",
    "\n",
    "        start_year = str(time_s[t])[0:4]\n",
    "        end_year   = str(time_f[t])[0:4]\n",
    "        print('   ', dsOut.time.values.min().astype('datetime64[h]'), dsOut.time.values.max().astype('datetime64[h]') )\n",
    "        dsOut.to_netcdf(out_dir + modelLs[i] + '_bias_corr_final_'+start_year+'-'+end_year+'.nc')\n",
    "        print('   ', out_dir + modelLs[i]+'_bias_corr_final_'+start_year+'-'+end_year+'.nc')\n",
    "        print('   merging and writing took ',time.time()-t1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e623adc-71ab-4326-8137-d4ab544d1c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e574db8d-1576-4613-b67c-8ac046f758f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "cannot serialize a bytes object larger than 4 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-85562c0394aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Call in parallel :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNprocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_to_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsFull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaCpQm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaSSTQm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_start\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_start\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-85562c0394aa>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Call in parallel :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNprocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_to_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsFull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaCpQm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaSSTQm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_start\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_start\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/yifanc/anaconda3/envs/py3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m    257\u001b[0m         '''\n\u001b[1;32m    258\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mRUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/yifanc/anaconda3/envs/py3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/yifanc/anaconda3/envs/py3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    422\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/yifanc/anaconda3/envs/py3/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/yifanc/anaconda3/envs/py3/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot serialize a bytes object larger than 4 GiB"
     ]
    }
   ],
   "source": [
    "\n",
    "Nprocs=4 #len(time_s) # abit much maybe\n",
    "\n",
    "## Call in parallel :\n",
    "with mp.Pool(processes = Nprocs) as p:\n",
    "    [p.apply(write_to_file, args=(dsFull, daCpQm, daSSTQm, time_start )) for time_start in time_s] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4312d075-4d96-460c-bca9-b2fece259611",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Generic BC (fut and past)\n",
    "\n",
    "for i in range(0,len(modelLs)):\n",
    "    for scen in scenarios:\n",
    "        t0 =time.time()\n",
    "        print(str(i+1)+' '+modelLs[i] + ' ' + scen)\n",
    "\n",
    "        # Load the 3D bc GCM data, onto which we will add the SST and cp \n",
    "        dsFull = xr.open_mfdataset(in_dir + modelLs[i] +'/'+ scen +'/'+modelLs[i] + '*.nc',combine='by_coords').load()\n",
    "\n",
    "        # Get SST and cp from the original (raw) GCM data:\n",
    "        dsRaw = xr.open_mfdataset(raw_dir + modelLs[i] +'/'+ scen +'/'+modelLs[i] + '*.nc',combine='by_coords')\n",
    "        dsCp_in = dsRaw['prec'] #.sel(time=slice(t1,t2)) ## WHY slice??? not generic. \n",
    "        dsSST_in = dsRaw['SST'] #.sel(time=slice(t1,t2))\n",
    "        # dsCp_in=dsCp_in.sel(time=~dsCp_in.get_index(\"time\").duplicated())\n",
    "        # dsSST_in=dsSST_in.sel(time=~dsSST_in.get_index(\"time\").duplicated())\n",
    "        print(\"   GCM data loaded\")\n",
    "\n",
    "\n",
    "\n",
    "        # Need to do a interpolation here with xesmf\n",
    "        cp_grid_with_bounds   = get_latlon_b_rect(dsCp_in,lon_str='lon',lat_str='lat',lon_dim='lon',lat_dim='lat')\n",
    "        # sst_grid_with_bounds  = get_latlon_b_rect(dsSST_in,lon_str='lon',lat_str='lat',lon_dim='lon',lat_dim='lat')         # redundant, should be same grid?\n",
    "        erai_grid_with_bounds = get_latlon_b_rect(dsObs['sst'],lon_str='lon',lat_str='lat',lon_dim='lon',lat_dim='lat')\n",
    "        regridder = xe.Regridder(cp_grid_with_bounds, erai_grid_with_bounds, 'bilinear') # input grid, gird you want to resample to, method\n",
    "\n",
    "        dsCp_out = regridder(dsCp_in).load()\n",
    "        dsSST_out = regridder(dsSST_in).load()\n",
    "\n",
    "        dsObs['cp']  = dsObs['cp'].load()\n",
    "        dsObs['sst'] = dsObs['sst'].load()\n",
    "\n",
    "        # the reference dataset should always be historical:\n",
    "        if scen=='historical':\n",
    "            dsCp_ref=dsCp_out.sel(time=slice('1979-09-01','2019-08-31'))\n",
    "            dsSST_ref=dsSST_out.sel(time=slice('1979-09-01','2019-08-31'))\n",
    "        else:  # load the historical GCM, regrid, ...\n",
    "            dsRaw_ref = xr.open_mfdataset(raw_dir + modelLs[i] +'/historical/'+modelLs[i] + '*.nc',combine='by_coords')\n",
    "            dsCp_ref =  regridder(dsRaw_ref['prec']).sel(time=slice('1979-09-01','2019-08-31')).load()\n",
    "            dsSST_ref =  regridder(dsRaw_ref['SST']).sel(time=slice('1979-09-01','2019-08-31')).load()\n",
    "          \n",
    "        dsSST_out=dsSST_out.interpolate_na(dim='time')\n",
    "        print(\"   Regridded\")\n",
    "\n",
    "        # Bias Correct Convective Precipitation\n",
    "        daPrecQm=qm.quantile_mapping_by_group(  dsCp_out,\n",
    "                                                dsCp_ref, # this should always be hist period?\n",
    "                                                dsObs['cp'].sel(time=slice('1979-09-01','2019-08-31')),\n",
    "                                                grouper='time.month',detrend=False,use_ref_data=True,extrapolate='1to1',n_endpoints=50)\n",
    "        daPrecQm = xr.where(dsCp_out <= 0, 0, daPrecQm)\n",
    "        daPrecQm.attrs = dsCp_in.attrs\n",
    "        daCpQm=daPrecQm.to_dataset(name='cp').load()\n",
    "        # check/ add attrs?\n",
    "        print(daPrecQm.attrs)\n",
    "        print(\"   BC cp\")\n",
    "\n",
    "\n",
    "        # Bias Correct Sea Surface Temperature\n",
    "        daSSTQm=qm.quantile_mapping_by_group(   dsSST_out,\n",
    "                                                dsSST_ref,\n",
    "                                                dsObs['sst'].sel(time=slice('1979-09-01','2019-08-31')),\n",
    "                                                grouper='time.month',detrend=True,use_ref_data=True,extrapolate='1to1',n_endpoints=50)\n",
    "        daSSTQm.attrs = dsSST_in.attrs\n",
    "        daSSTQm=daSSTQm.to_dataset(name='tskin').load()\n",
    "        print(\"   BC SST\")\n",
    "        print(\"   total BC time: \", time.time()-t0, \"\\n\") \n",
    "        \n",
    "        if scen == 'historical':\n",
    "            time_s = np.arange(1950, 2015, 10)\n",
    "            time_f = np.arange(1959, 2099, 10)\n",
    "        else:\n",
    "            time_s = np.arange(2015, 2099, 10)\n",
    "            time_f = np.arange(2024, 2100, 10)\n",
    "\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "\n",
    "        Nprocs=4 #len(time_s) # abit much maybe\n",
    "   \n",
    "        ## Call in parallel :\n",
    "        with mp.Pool(processes = Nprocs) as p:\n",
    "            [p.apply(write_to_file, args=(dsFull, daCpQm, daSSTQm, time_start )) for time_start in time_s] \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "#             # save the result in 10 year chunks:\n",
    "#         for t in range(0,len(time_s)):\n",
    "           \n",
    "#             t1=time.time()\n",
    "\n",
    "#             dsOut=xr.merge([#dsGCM_bc.sel(time=slice(time_s[t],time_f[t])),\n",
    "#                             dsFull.sel(time=slice(str(time_s[t]),str(time_f[t]))),\n",
    "#                             daCpQm.sel(time=slice(str(time_s[t]),str(time_f[t]))),\n",
    "#                             daSSTQm.sel(time=slice(str(time_s[t]),str(time_f[t])))])\n",
    "\n",
    "#             start_year = str(time_s[t])[0:4]\n",
    "#             end_year   = str(time_f[t])[0:4]\n",
    "#             print('   ', dsOut.time.values.min().astype('datetime64[h]'), dsOut.time.values.max().astype('datetime64[h]') )\n",
    "#             dsOut.to_netcdf(out_dir + modelLs[i] + '_bias_corr_final_'+start_year+'-'+end_year+'.nc')\n",
    "#             print('   ', out_dir + modelLs[i]+'_bias_corr_final_'+start_year+'-'+end_year+'.nc')\n",
    "#             print('   merging and writing took ',time.time()-t1 )\n",
    "                  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e605a6b-0740-44d6-9641-288ab33d46d0",
   "metadata": {},
   "source": [
    "### Original below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd042d-33f0-43b2-b9fb-a3eb520c644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,len(modelLs)):\n",
    "    \n",
    "    print(str(i+1)+' '+modelLs[i])\n",
    "\n",
    "    # print(modelLs[i]+' ssp585')\n",
    "    \n",
    "    # Convective Precipitation\n",
    "    # dsFull = xr.open_mfdataset(in_dir + modelLs[i] + '*.nc',combine='by_coords').load()\n",
    "    dsFull = xr.open_mfdataset(in_dir + modelLs[i] +'/'+ scen +'/'+modelLs[i] + '*.nc',combine='by_coords').load()\n",
    "    dsCp = dsFull['prec'].sel(time=slice(t1,t2))\n",
    "\n",
    "    # Sea Surface Temperature\n",
    "    dsGCM_b4_bc = dsFull['SST'].sel(time=slice(t1,t2))\n",
    "    dsGCM_b4_bc=dsGCM_b4_bc.sel(time=~dsGCM_b4_bc.get_index(\"time\").duplicated())\n",
    "    print(\"Raw data loaded\")\n",
    "\n",
    "    # To store the output data\n",
    "    files=sorted(glob.glob('/glade/scratch/abby/ESM_bias_correction/ref_period_only/bias_corrected_merged_' + modelLs[i] + '*.nc'))\n",
    "    dsGCM_bc=xr.open_mfdataset(files,combine='by_coords').sel(time=slice(t1,t2))\n",
    "    dsGCM_bc=dsGCM_bc.sel(time=~dsGCM_bc.get_index(\"time\").duplicated()) #remove any possible duplicate value\n",
    "    print(\"BC data loaded\")\n",
    "    \n",
    "    # Need to do a interpolation here with xesmf\n",
    "    cp_grid_with_bounds   = get_latlon_b_rect(dsCp,lon_str='lon',lat_str='lat',lon_dim='lon',lat_dim='lat')\n",
    "    sst_grid_with_bounds  = get_latlon_b_rect(dsGCM_b4_bc,lon_str='lon',lat_str='lat',lon_dim='lon',lat_dim='lat')\n",
    "    erai_grid_with_bounds = get_latlon_b_rect(dsObs['sst'],lon_str='lon',lat_str='lat',lon_dim='lon',lat_dim='lat')\n",
    "\n",
    "    regridder = xe.Regridder(cp_grid_with_bounds, erai_grid_with_bounds, 'bilinear') # input grid, gird you want to resample to, method\n",
    "    dsCp_out = regridder(dsCp)\n",
    "\n",
    "    regridder = xe.Regridder(sst_grid_with_bounds, erai_grid_with_bounds, 'bilinear') # input grid, gird you want to resample to, method\n",
    "    dsSST_out = regridder(dsGCM_b4_bc)\n",
    "\n",
    "    dsObs['cp']  = dsObs['cp'].load()\n",
    "    dsCp_out     = dsCp_out.load()\n",
    "    dsObs['sst'] = dsObs['sst'].load()\n",
    "    dsSST_out    = dsSST_out.load()\n",
    "    \n",
    "    dsSST_out=dsSST_out.interpolate_na(dim='time')\n",
    "    print(\"Regridded\")\n",
    "    # Bias Correct Convective Precipitation\n",
    "    \n",
    "    # add n endpoints = 50 to both\n",
    "    daPrecQm=qm.quantile_mapping_by_group(dsCp_out,\n",
    "                                                dsCp_out.sel(time=slice('1979-09-01','2019-08-31')),\n",
    "                                                dsObs['cp'].sel(time=slice('1979-09-01','2019-08-31')),\n",
    "                                                grouper='time.month',detrend=False,use_ref_data=True,extrapolate='1to1',n_endpoints=50)\n",
    "    daPrecQm = xr.where(dsCp_out <= 0, 0, daPrecQm)\n",
    "    daCpQm=daPrecQm.to_dataset(name='cp')\n",
    "    print(\"BC cp\")\n",
    "\n",
    "\n",
    "    # Bias Correct Sea Surface Temperature\n",
    "    daSSTQm=qm.quantile_mapping_by_group(dsSST_out,\n",
    "                                                dsSST_out.sel(time=slice('1979-09-01','2019-08-31')),\n",
    "                                                dsObs['sst'].sel(time=slice('1979-09-01','2019-08-31')),\n",
    "                                                grouper='time.month',detrend=True,use_ref_data=True,extrapolate='1to1',n_endpoints=50)\n",
    "    daSSTQm=daSSTQm.to_dataset(name='tskin')\n",
    "    print(\"BC SST\")\n",
    "\n",
    "\n",
    "    time_s=['1950-01-01T12:00','1960-01-01T00:00','1970-01-01T00:00','1980-01-01T00:00','1990-01-01T00:00',\n",
    "            '2000-01-01T00:00','2010-01-01T00:00']\n",
    "\n",
    "    time_f=['1959-12-31T18:00','1969-12-31T18:00','1979-12-31T18:00','1989-12-31T18:00','1999-12-31T18:00',\n",
    "            '2009-12-31T18:00','2019-08-31T18:00']\n",
    "\n",
    "\n",
    "    for t in range(0,len(time_s)):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import os\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        dsOut=xr.merge([dsGCM_bc.sel(time=slice(time_s[t],time_f[t])),\n",
    "                        daCpQm.sel(time=slice(time_s[t],time_f[t])),\n",
    "                        daSSTQm.sel(time=slice(time_s[t],time_f[t]))])\n",
    "\n",
    "        start_year = time_s[t][0:4]\n",
    "        end_year   = time_f[t][0:4]\n",
    "        print(out_dir+'/'+modelLs[i]+'_bias_corr_final_'+start_year+'-'+end_year+'.nc')\n",
    "        dsOut.to_netcdf(out_dir+'/'+modelLs[i]+'_bias_corr_final_'+start_year+'-'+end_year+'.nc')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_yifan",
   "language": "python",
   "name": "py3_yifan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
